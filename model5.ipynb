{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43d218ca-d8fb-4bc3-b691-af673b6df9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: facenet-pytorch in c:\\users\\daara\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\daara\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: torch in c:\\users\\daara\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in c:\\users\\daara\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.17.2)\n",
      "Requirement already satisfied: Pillow<10.3.0,>=10.2.0 in c:\\users\\daara\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from facenet-pytorch) (10.2.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\daara\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from facenet-pytorch) (2.32.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.0.0 in c:\\users\\daara\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from facenet-pytorch) (4.67.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.24.0 in c:\\users\\daara\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from facenet-pytorch) (1.24.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\daara\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: sympy in c:\\users\\daara\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\daara\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\daara\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\daara\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\daara\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\daara\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2025.4.26)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\daara\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2.4.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\daara\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\daara\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.10)\n",
      "Requirement already satisfied: colorama in c:\\users\\daara\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm<5.0.0,>=4.0.0->facenet-pytorch) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\daara\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\daara\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\daara\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\daara\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\daara\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\daara\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\daara\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\daara\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install facenet-pytorch opencv-python torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2374efa2-80ea-4b86-9d44-c6b654463d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Face Recognition System ===\n",
      "\n",
      "=== Loading Database ===\n",
      "\n",
      "Processing Amar:\n",
      "  ✓ Amar_1.jpg: Success\n",
      "  ✗ person1_2.jpg: No face detected\n",
      "\n",
      "Processing Arpita_singh:\n",
      "  ✓ Arpita_singh.png: Success\n",
      "\n",
      "Processing person1:\n",
      "  ✗ sample.jpg: No face detected\n",
      "\n",
      "Processing person2:\n",
      "  ✓ person2_1.jpg: Success\n",
      "  ✓ person2_1749916628.jpg: Success\n",
      "\n",
      "Processing ronaldo:\n",
      "  ✓ ronaldo_1.jpg: Success\n",
      "\n",
      "=== Database Summary ===\n",
      "Loaded 4 people\n",
      "- Amar: 1 samples\n",
      "- Arpita_singh: 1 samples\n",
      "- person2: 2 samples\n",
      "- ronaldo: 1 samples\n",
      "\n",
      "Starting recognition... (Press Q to quit)\n",
      "Match: person2 (Confidence: 0.10)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: person2 (Confidence: 0.03)\n",
      "Match: person2 (Confidence: 0.13)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: person2 (Confidence: 0.02)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: person2 (Confidence: 0.19)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: person2 (Confidence: 0.17)\n",
      "Match: person2 (Confidence: 0.17)\n",
      "Match: person2 (Confidence: 0.12)\n",
      "Match: person2 (Confidence: 0.15)\n",
      "Match: person2 (Confidence: 0.01)\n",
      "Match: person2 (Confidence: 0.12)\n",
      "Match: person2 (Confidence: 0.03)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: person2 (Confidence: 0.10)\n",
      "Match: person2 (Confidence: 0.05)\n",
      "Match: person2 (Confidence: 0.11)\n",
      "Match: person2 (Confidence: 0.05)\n",
      "Match: person2 (Confidence: 0.02)\n",
      "Match: person2 (Confidence: 0.09)\n",
      "Match: person2 (Confidence: 0.08)\n",
      "Match: person2 (Confidence: 0.03)\n",
      "Match: person2 (Confidence: 0.07)\n",
      "Match: person2 (Confidence: 0.10)\n",
      "Match: person2 (Confidence: 0.13)\n",
      "Match: person2 (Confidence: 0.08)\n",
      "Match: person2 (Confidence: 0.09)\n",
      "Match: person2 (Confidence: 0.06)\n",
      "Match: Amar (Confidence: 0.06)\n",
      "Match: person2 (Confidence: 0.04)\n",
      "Match: person2 (Confidence: 0.02)\n",
      "Match: Amar (Confidence: 0.09)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: person2 (Confidence: 0.13)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: person2 (Confidence: 0.15)\n",
      "Match: person2 (Confidence: 0.07)\n",
      "Match: person2 (Confidence: 0.12)\n",
      "Match: person2 (Confidence: 0.01)\n",
      "Match: person2 (Confidence: 0.10)\n",
      "Match: person2 (Confidence: 0.02)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: person2 (Confidence: 0.05)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: person2 (Confidence: 0.06)\n",
      "Match: person2 (Confidence: 0.16)\n",
      "Match: person2 (Confidence: 0.13)\n",
      "Match: person2 (Confidence: 0.12)\n",
      "Match: person2 (Confidence: 0.06)\n",
      "Match: person2 (Confidence: 0.08)\n",
      "Match: person2 (Confidence: 0.07)\n",
      "Match: person2 (Confidence: 0.13)\n",
      "Match: Amar (Confidence: 0.08)\n",
      "Match: person2 (Confidence: 0.01)\n",
      "Match: person2 (Confidence: 0.12)\n",
      "Match: person2 (Confidence: 0.05)\n",
      "Match: person2 (Confidence: 0.09)\n",
      "Match: person2 (Confidence: 0.06)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: person2 (Confidence: 0.08)\n",
      "Match: person2 (Confidence: 0.06)\n",
      "Match: person2 (Confidence: 0.07)\n",
      "Match: person2 (Confidence: 0.01)\n",
      "Match: person2 (Confidence: 0.11)\n",
      "Match: person2 (Confidence: 0.14)\n",
      "Match: person2 (Confidence: 0.05)\n",
      "Match: person2 (Confidence: 0.14)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: Amar (Confidence: 0.02)\n",
      "Match: Amar (Confidence: 0.04)\n",
      "Match: person2 (Confidence: 0.16)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Recognition error: torch.cat(): expected a non-empty list of Tensors\n",
      "Match: person2 (Confidence: 0.17)\n",
      "Match: person2 (Confidence: 0.14)\n",
      "Match: person2 (Confidence: 0.20)\n",
      "Match: person2 (Confidence: 0.13)\n",
      "Match: person2 (Confidence: 0.10)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: Amar (Confidence: 0.13)\n",
      "Match: person2 (Confidence: 0.13)\n",
      "Match: person2 (Confidence: 0.16)\n",
      "Match: Amar (Confidence: 0.11)\n",
      "Match: person2 (Confidence: 0.13)\n",
      "Match: person2 (Confidence: 0.13)\n",
      "Match: person2 (Confidence: 0.04)\n",
      "Match: person2 (Confidence: 0.07)\n",
      "Match: Amar (Confidence: 0.09)\n",
      "Match: person2 (Confidence: 0.09)\n",
      "Match: person2 (Confidence: 0.13)\n",
      "Match: person2 (Confidence: 0.08)\n",
      "Match: person2 (Confidence: 0.11)\n",
      "Match: person2 (Confidence: 0.07)\n",
      "Match: Amar (Confidence: 0.02)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: person2 (Confidence: 0.07)\n",
      "Match: person2 (Confidence: 0.04)\n",
      "Match: person2 (Confidence: 0.02)\n",
      "Match: person2 (Confidence: 0.04)\n",
      "Match: person2 (Confidence: 0.07)\n",
      "Match: person2 (Confidence: 0.03)\n",
      "Match: person2 (Confidence: 0.01)\n",
      "Match: Amar (Confidence: 0.08)\n",
      "Match: person2 (Confidence: 0.09)\n",
      "Match: Amar (Confidence: 0.01)\n",
      "Match: person2 (Confidence: 0.01)\n",
      "Match: person2 (Confidence: 0.04)\n",
      "Match: Amar (Confidence: 0.03)\n",
      "Match: person2 (Confidence: 0.02)\n",
      "Match: Amar (Confidence: 0.02)\n",
      "Match: Amar (Confidence: 0.14)\n",
      "Match: person2 (Confidence: 0.00)\n",
      "Match: Amar (Confidence: 0.03)\n",
      "Match: Amar (Confidence: 0.08)\n",
      "Match: Amar (Confidence: 0.04)\n",
      "Match: Amar (Confidence: 0.04)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: Amar (Confidence: 0.00)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: Amar (Confidence: 0.02)\n",
      "Match: Amar (Confidence: 0.06)\n",
      "Match: Amar (Confidence: 0.04)\n",
      "Match: Amar (Confidence: 0.00)\n",
      "Match: person2 (Confidence: 0.02)\n",
      "Match: person2 (Confidence: 0.01)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: person2 (Confidence: 0.03)\n",
      "Match: person2 (Confidence: 0.03)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "Match: person2 (Confidence: 0.05)\n",
      "Match: Amar (Confidence: 0.04)\n",
      "Match: Amar (Confidence: 0.08)\n",
      "Match: person2 (Confidence: 0.03)\n",
      "Match: person2 (Confidence: 0.10)\n",
      "Match: person2 (Confidence: 0.08)\n",
      "Match: person2 (Confidence: 0.03)\n",
      "Match: person2 (Confidence: 0.09)\n",
      "Match: Amar (Confidence: 0.10)\n",
      "Match: person2 (Confidence: 0.08)\n",
      "Match: person2 (Confidence: 0.05)\n",
      "Match: person2 (Confidence: 0.05)\n",
      "Match: person2 (Confidence: 0.09)\n",
      "Match: person2 (Confidence: 0.11)\n",
      "Match: Amar (Confidence: 0.06)\n",
      "Match: person2 (Confidence: 0.06)\n",
      "Match: Unknown (Confidence: 0.00)\n",
      "System stopped\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from PIL import Image\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class FaceRecognition:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Properly initialized MTCNN\n",
    "        self.mtcnn = MTCNN(\n",
    "            image_size=160,\n",
    "            margin=20,\n",
    "            min_face_size=40,\n",
    "            thresholds=[0.6, 0.7, 0.7],\n",
    "            factor=0.709,\n",
    "            post_process=True,\n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        # Properly initialized Resnet\n",
    "        self.resnet = InceptionResnetV1(\n",
    "            pretrained='vggface2',\n",
    "            classify=False\n",
    "        ).eval().to(self.device)\n",
    "        \n",
    "        self.known_embeddings = {}\n",
    "        self.known_names = []\n",
    "\n",
    "    def load_database(self, db_path=\"known_faces\"):\n",
    "        print(\"\\n=== Loading Database ===\")\n",
    "        \n",
    "        # Skip hidden directories\n",
    "        person_dirs = [d for d in os.listdir(db_path) if not d.startswith('.')]\n",
    "        \n",
    "        for person_name in person_dirs:\n",
    "            person_dir = os.path.join(db_path, person_name)\n",
    "            if not os.path.isdir(person_dir):\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\nProcessing {person_name}:\")\n",
    "            embeddings = []\n",
    "            \n",
    "            image_files = [f for f in os.listdir(person_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "            \n",
    "            for img_file in image_files:\n",
    "                img_path = os.path.join(person_dir, img_file)\n",
    "                try:\n",
    "                    img = Image.open(img_path).convert('RGB')\n",
    "                    \n",
    "                    # Detect face and get properly aligned face tensor\n",
    "                    face = self.mtcnn(img)\n",
    "                    if face is None:\n",
    "                        print(f\"  ✗ {img_file}: No face detected\")\n",
    "                        continue\n",
    "                        \n",
    "                    # Ensure proper tensor dimensions [1, 3, 160, 160]\n",
    "                    if face.dim() == 3:\n",
    "                        face = face.unsqueeze(0)\n",
    "                    \n",
    "                    # Generate embedding\n",
    "                    embedding = self.resnet(face.to(self.device)).detach().cpu()\n",
    "                    embeddings.append(embedding)\n",
    "                    print(f\"  ✓ {img_file}: Success\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  ✗ {img_file}: Error - {str(e)}\")\n",
    "            \n",
    "            if embeddings:\n",
    "                self.known_embeddings[person_name] = torch.cat(embeddings)\n",
    "                self.known_names.append(person_name)\n",
    "        \n",
    "        print(\"\\n=== Database Summary ===\")\n",
    "        print(f\"Loaded {len(self.known_names)} people\")\n",
    "        for name in self.known_names:\n",
    "            print(f\"- {name}: {self.known_embeddings[name].shape[0]} samples\")\n",
    "\n",
    "    def recognize(self, frame):\n",
    "        try:\n",
    "            img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            \n",
    "            # Detect faces\n",
    "            boxes, probs, landmarks = self.mtcnn.detect(img, landmarks=True)\n",
    "            if boxes is None:\n",
    "                return []\n",
    "            \n",
    "            results = []\n",
    "            for i, (box, prob) in enumerate(zip(boxes, probs)):\n",
    "                if prob < 0.9:\n",
    "                    continue\n",
    "                    \n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                face_img = img.crop((x1, y1, x2, y2))\n",
    "                \n",
    "                # Get aligned face\n",
    "                face = self.mtcnn(face_img)\n",
    "                if face is None:\n",
    "                    continue\n",
    "                    \n",
    "                # Ensure proper tensor dimensions\n",
    "                if face.dim() == 3:\n",
    "                    face = face.unsqueeze(0)\n",
    "                \n",
    "                # Generate embedding\n",
    "                embedding = self.resnet(face.to(self.device)).detach().cpu()\n",
    "                \n",
    "                # Find matches\n",
    "                name, confidence = self._match_face(embedding)\n",
    "                print(f\"Match: {name} (Confidence: {confidence:.2f})\")\n",
    "                \n",
    "                results.append({\n",
    "                    'box': box,\n",
    "                    'name': name,\n",
    "                    'confidence': confidence\n",
    "                })\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Recognition error: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def _match_face(self, embedding):\n",
    "        if not self.known_embeddings:\n",
    "            return \"Unknown\", 0.0\n",
    "            \n",
    "        min_dist = float('inf')\n",
    "        best_name = \"Unknown\"\n",
    "        \n",
    "        for name, ref_embeddings in self.known_embeddings.items():\n",
    "            dists = torch.cdist(embedding, ref_embeddings)\n",
    "            current_min = dists.min().item()\n",
    "            \n",
    "            if current_min < min_dist:\n",
    "                min_dist = current_min\n",
    "                best_name = name\n",
    "        \n",
    "        threshold = 0.7  # Optimal threshold for face recognition\n",
    "        confidence = max(0.0, 1.0 - min(min_dist / threshold, 1.0))\n",
    "        \n",
    "        return (best_name, confidence) if min_dist < threshold else (\"Unknown\", 0.0)\n",
    "\n",
    "def main():\n",
    "    print(\"=== Face Recognition System ===\")\n",
    "    recognizer = FaceRecognition()\n",
    "    \n",
    "    # Load database\n",
    "    try:\n",
    "        recognizer.load_database(\"known_faces\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load database: {str(e)}\")\n",
    "        return\n",
    "    \n",
    "    # Initialize camera\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Cannot open camera\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nStarting recognition... (Press Q to quit)\")\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to capture frame\")\n",
    "            break\n",
    "        \n",
    "        frame = cv2.flip(frame, 1)\n",
    "        faces = recognizer.recognize(frame)\n",
    "        \n",
    "        # Draw results\n",
    "        for face in faces:\n",
    "            x1, y1, x2, y2 = map(int, face['box'])\n",
    "            color = (0, 255, 0) if face['name'] != \"Unknown\" else (0, 0, 255)\n",
    "            label = f\"{face['name']} ({face['confidence']:.2f})\"\n",
    "            \n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(frame, label, (x1, y1-10), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "        \n",
    "        cv2.imshow(\"Face Recognition\", frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"System stopped\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a1f1bd5-c16e-487d-ab37-d50ada641a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daara\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Face Recognition System ===\n",
      "\n",
      "=== Loading Database ===\n",
      "\n",
      "Processing Amar:\n",
      "  ✓ Amar_1.jpg: Success\n",
      "  ✗ person1_2.jpg: No face detected\n",
      "\n",
      "Processing Arpita_singh:\n",
      "  ✓ Arpita_singh.png: Success\n",
      "\n",
      "Processing person1:\n",
      "  ✗ sample.jpg: No face detected\n",
      "\n",
      "Processing person2:\n",
      "  ✓ person2_1.jpg: Success\n",
      "  ✓ person2_1749916628.jpg: Success\n",
      "\n",
      "Processing ronaldo:\n",
      "  ✓ ronaldo_1.jpg: Success\n",
      "\n",
      "=== Database Summary ===\n",
      "Loaded 4 people\n",
      "\n",
      "Starting recognition... (Press Q to quit)\n",
      "System stopped\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from PIL import Image\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class FaceRecognition:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.mtcnn = MTCNN(image_size=160, margin=20, min_face_size=40, \n",
    "                          thresholds=[0.6, 0.7, 0.7], device=self.device)\n",
    "        self.resnet = InceptionResnetV1(pretrained='vggface2', classify=False).eval().to(self.device)\n",
    "        self.known_embeddings = {}\n",
    "        self.known_names = []\n",
    "        self.recognition_history = []\n",
    "        self.frame_count = 0\n",
    "        self.fps = 0\n",
    "        self.last_time = datetime.now()\n",
    "\n",
    "    def load_database(self, db_path=\"known_faces\"):\n",
    "        print(\"\\n=== Loading Database ===\")\n",
    "        person_dirs = [d for d in os.listdir(db_path) if not d.startswith('.')]\n",
    "        \n",
    "        for person_name in person_dirs:\n",
    "            person_dir = os.path.join(db_path, person_name)\n",
    "            if not os.path.isdir(person_dir):\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\nProcessing {person_name}:\")\n",
    "            embeddings = []\n",
    "            \n",
    "            image_files = [f for f in os.listdir(person_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "            \n",
    "            for img_file in image_files:\n",
    "                img_path = os.path.join(person_dir, img_file)\n",
    "                try:\n",
    "                    img = Image.open(img_path).convert('RGB')\n",
    "                    face = self.mtcnn(img)\n",
    "                    if face is None:\n",
    "                        print(f\"  ✗ {img_file}: No face detected\")\n",
    "                        continue\n",
    "                        \n",
    "                    if face.dim() == 3:\n",
    "                        face = face.unsqueeze(0)\n",
    "                    \n",
    "                    embedding = self.resnet(face.to(self.device)).detach().cpu()\n",
    "                    embeddings.append(embedding)\n",
    "                    print(f\"  ✓ {img_file}: Success\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  ✗ {img_file}: Error - {str(e)}\")\n",
    "            \n",
    "            if embeddings:\n",
    "                self.known_embeddings[person_name] = torch.cat(embeddings)\n",
    "                self.known_names.append(person_name)\n",
    "        \n",
    "        print(\"\\n=== Database Summary ===\")\n",
    "        print(f\"Loaded {len(self.known_names)} people\")\n",
    "\n",
    "    def recognize(self, frame):\n",
    "        try:\n",
    "            self.frame_count += 1\n",
    "            current_time = datetime.now()\n",
    "            if (current_time - self.last_time).seconds >= 1:\n",
    "                self.fps = self.frame_count\n",
    "                self.frame_count = 0\n",
    "                self.last_time = current_time\n",
    "            \n",
    "            img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            boxes, probs, _ = self.mtcnn.detect(img, landmarks=True)\n",
    "            \n",
    "            results = []\n",
    "            if boxes is not None:\n",
    "                for i, (box, prob) in enumerate(zip(boxes, probs)):\n",
    "                    if prob < 0.9:\n",
    "                        continue\n",
    "                        \n",
    "                    x1, y1, x2, y2 = map(int, box)\n",
    "                    face_img = img.crop((x1, y1, x2, y2))\n",
    "                    face = self.mtcnn(face_img)\n",
    "                    \n",
    "                    if face is not None:\n",
    "                        if face.dim() == 3:\n",
    "                            face = face.unsqueeze(0)\n",
    "                        \n",
    "                        embedding = self.resnet(face.to(self.device)).detach().cpu()\n",
    "                        name, confidence = self._match_face(embedding)\n",
    "                        \n",
    "                        if name != \"Unknown\":\n",
    "                            self.recognition_history.append((name, confidence))\n",
    "                            if len(self.recognition_history) > 5:\n",
    "                                self.recognition_history.pop(0)\n",
    "                        \n",
    "                        results.append({\n",
    "                            'box': box,\n",
    "                            'name': name,\n",
    "                            'confidence': confidence\n",
    "                        })\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Recognition error: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def _match_face(self, embedding):\n",
    "        if not self.known_embeddings:\n",
    "            return \"Unknown\", 0.0\n",
    "            \n",
    "        min_dist = float('inf')\n",
    "        best_name = \"Unknown\"\n",
    "        \n",
    "        for name, ref_embeddings in self.known_embeddings.items():\n",
    "            dists = torch.cdist(embedding, ref_embeddings)\n",
    "            current_min = dists.min().item()\n",
    "            \n",
    "            if current_min < min_dist:\n",
    "                min_dist = current_min\n",
    "                best_name = name\n",
    "        \n",
    "        threshold = 0.7\n",
    "        confidence = max(0.0, 1.0 - min(min_dist / threshold, 1.0))\n",
    "        \n",
    "        return (best_name, confidence) if min_dist < threshold else (\"Unknown\", 0.0)\n",
    "\n",
    "def draw_info_panel(frame, recognizer, width=300):\n",
    "    \"\"\"Draw attractive information panel on the left side\"\"\"\n",
    "    height = frame.shape[0]\n",
    "    panel = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Gradient background\n",
    "    cv2.rectangle(panel, (0, 0), (width, height), (40, 40, 60), -1)\n",
    "    for i in range(height):\n",
    "        alpha = i/height\n",
    "        color = tuple(int(40 + alpha*40) for _ in range(3))\n",
    "        cv2.line(panel, (0, i), (width, i), color, 1)\n",
    "    \n",
    "    # System info\n",
    "    cv2.putText(panel, \"FACE RECOGNITION SYSTEM\", (10, 30), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 200, 255), 2)\n",
    "    \n",
    "    # FPS counter\n",
    "    cv2.putText(panel, f\"FPS: {recognizer.fps}\", (10, 70), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 1)\n",
    "    \n",
    "    # Database info\n",
    "    cv2.putText(panel, f\"Known Faces: {len(recognizer.known_names)}\", (10, 110), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 1)\n",
    "    \n",
    "    # Recent recognitions\n",
    "    cv2.putText(panel, \"Recent Recognitions:\", (10, 160), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 200, 255), 1)\n",
    "    \n",
    "    for i, (name, conf) in enumerate(recognizer.recognition_history):\n",
    "        y_pos = 190 + i * 30\n",
    "        cv2.putText(panel, f\"{name}: {conf:.2f}\", (20, y_pos), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "    \n",
    "    # Instructions\n",
    "    cv2.putText(panel, \"Instructions:\", (10, height-100), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 0), 1)\n",
    "    cv2.putText(panel, \"Press 'Q' to quit\", (20, height-70), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)\n",
    "    \n",
    "    # Add logo or decorative elements\n",
    "    cv2.rectangle(panel, (width//2-30, height-40), (width//2+30, height-20), \n",
    "                  (0, 150, 255), -1)\n",
    "    cv2.putText(panel, \"AI\", (width//2-15, height-25), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "    \n",
    "    return np.hstack((panel, frame))\n",
    "\n",
    "def main():\n",
    "    print(\"=== Face Recognition System ===\")\n",
    "    recognizer = FaceRecognition()\n",
    "    \n",
    "    try:\n",
    "        recognizer.load_database(\"known_faces\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load database: {str(e)}\")\n",
    "        return\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Cannot open camera\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nStarting recognition... (Press Q to quit)\")\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to capture frame\")\n",
    "            break\n",
    "        \n",
    "        frame = cv2.flip(frame, 1)\n",
    "        faces = recognizer.recognize(frame)\n",
    "        \n",
    "        for face in faces:\n",
    "            x1, y1, x2, y2 = map(int, face['box'])\n",
    "            color = (0, 255, 0) if face['name'] != \"Unknown\" else (0, 0, 255)\n",
    "            label = f\"{face['name']} ({face['confidence']:.2f})\"\n",
    "            \n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(frame, label, (x1, y1-10), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "        \n",
    "        # Add information panel\n",
    "        frame = draw_info_panel(frame, recognizer)\n",
    "        \n",
    "        cv2.imshow(\"Face Recognition System\", frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"System stopped\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4169244b-1bc8-49ba-a66a-e1ccfc0e75bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
